# -*- coding: utf-8 -*-
"""Project_ProbStats_B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aBBxBoFAhFEVXlkabLj5vvbqr5c-KmtW
"""

import pandas as pd
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
# %cd D:\StonyBrook\Study\Prob&Stats CSE544\Project

# from google.colab import drive
# drive.mount('/content/gdrive')

# %cd /content/gdrive/My Drive/Prob_stats_proj

"""## Data processing"""

data = pd.read_csv('7.csv')

## converting date column to datetime data type ##
data['Date'] = pd.to_datetime(data['Date'])

data

data.info()

###### Feb and March data #######
feb_start_date , feb_end_date = '2021-02-01', '2021-02-28'
march_start_date, march_end_date = '2021-03-01', '2021-03-31'
condition = (data['Date'] >= feb_start_date) & (data['Date'] <= feb_end_date)
feb_data = data.loc[condition]

condition = (data['Date'] >= march_start_date) & (data['Date'] <= march_end_date)
march_data = data.loc[condition]

print(len(march_data))

"""## Hypothesis tests for IA confirmed cases
The below hypothesis tests are on IA confirmed cases
"""

########################### Tests for IA confirmed cases handle ###################################

"""### 1. Wald's Test for IA_confirmed
The null hypothesis for the Wald's test below is: 

average_IA_confirmed_in_feb = average_IA_confirmed_in_march

The wald's statistic obtained is less than the critical value, hence we accept the above hypothesis

Is Wald's test applicable?
Since the number of samples is high, using CLT the mean is Asymptotically normal. Hence wald's test is applicable
"""

handle = 'IA confirmed'
mean_feb = np.mean(feb_data[handle])

## Using mean_feb as guess for mean_march ##

# Wald's test # 
# w = (theta_hat - theta_0)/ se_hat(theta_hat)
# theta_hat is estimator of theta

# Null Hypothesis : mean_march = mean_feb
# Alternate Hypothesis : mean_march != mean_feb

# Assuming the distribution of march data to be poisson. MLE_mean = Sample_mean

sample_mean_march = np.mean(march_data[handle])

standard_error_estimate = np.sqrt(sample_mean_march/len(march_data))

walds_statistic = np.abs((sample_mean_march - mean_feb)/standard_error_estimate)

print('MLE for March data ',sample_mean_march )
print('Guess mean ', mean_feb)
print('Standard Error ', standard_error_estimate)
print('Walds Statistic', walds_statistic )

# Accept Null Hypothesis(less than 1.96)

"""### 2. Z-Test for IA_confirmed
The null hypothesis for the z-test is same as the wald's test above

We accept the null hypothesis in this case as it is less than the critical value.

Is Z-test applicable?
Yes, since we estimate true variance using the entire data, and number of samples is large
"""

## Z-test ##

# z_statistic = (sample_mean - guess)/ root(true_variance/n)
# true variance = corrected sample standard deviation

sample_mean_full_data = np.mean(data[handle])

true_variance = np.sum(np.square(data[handle] - sample_mean_full_data))/(len(data)-1)

z_statistic = np.abs((sample_mean_march - mean_feb)/(np.sqrt(true_variance)/np.sqrt(len(march_data))))

print(' True Variance ', true_variance)
print(' Sample Mean ',sample_mean_march )
print(' Guess ', mean_feb)
print(' z_statistic ', z_statistic)


# Accept Null Hypothesis(less than 1.96)

"""### 3. T- Test for IA_confirmed

The null hypothesis for the T-test is same as the wald's test above

We accept the null hypothesis under the T-test as the T-statistic is less than the critical value. 

Is T-test applicable?
Since the number of samples is high, using CLT, the mean is Asymptotically normal. Hence T-test is applicable
"""

## t-test ##

# t_statistic = (sample_mean - guess)/ corrected_sample_standard_deviation/root(n)
corrected_sample_SD = np.sqrt(np.sum(np.square(march_data[handle] - sample_mean_march))/(len(march_data)-1))

t_statistic = np.abs((sample_mean_march - mean_feb)/(corrected_sample_SD/np.sqrt(len(march_data))))

print(' Corrected Sample Standard Deviation', corrected_sample_SD)
print(' Sample Mean ',sample_mean_march )
print(' Guess ', mean_feb)
print(' t_statistic ', t_statistic)

# degrees of freedom : 28 + 31 -2 = 57
# threshold = 2.002465

# Accept Null Hypothesis

"""### 4. Wald's 2 sample test

The null hypothesis for the this is same as the wald's test above

We accept the null hypothesis under Wald's 2 test as the statistic is less than the critical value.

Is Wald's 2 sample test applicable?
Since the number of samples is high, using CLT the mean is Asymptotically normal. Hence wald's 2 sample test is applicable
"""

## Walds - 2 sample test ##

# delta = mean_march - mean_feb 

# w_stat = delta_hat/ SE_hat(delta_hat)

# Assumption : Data is poisson distributed

# Null Hypothesis : delta = 0
# Alternate Hypothesis : delta != 0 


sample_mean_march = np.mean(march_data[handle])
sample_mean_feb = np.mean(feb_data[handle])

delta_hat = np.abs(sample_mean_march - sample_mean_feb)

SE_hat = np.sqrt(sample_mean_march/len(march_data) + sample_mean_feb/len(feb_data))

w_stat = np.abs(delta_hat/SE_hat)

print(' Standard Error Estimate', SE_hat)
print(' Delta Estimate ',delta_hat )
print(' w_stat ', w_stat)

# Accept Null Hypothesis

"""### 5. Unpaired 2-sample T-test
We accept the null hypothesis under this test as is less than the critical value

Is 2 sample T-test applicable?
Since the number of samples is high, using CLT, the mean is Asymptotically normal. Hence 2-sample T-test is applicable
"""

## 2 sample unpaired t-test ##

# D_bar = X_bar - Y_bar
# t_stat = D_bar/root(corrected_var_x/n + corrected_var_y/m)

corrected_variance_march = np.sum(np.square(march_data[handle] - sample_mean_march))/(len(march_data)-1)
corrected_variance_feb =  np.sum(np.square(feb_data[handle] - sample_mean_feb))/(len(feb_data)-1)

D_bar = np.abs(sample_mean_march - sample_mean_feb)

t_stat = D_bar/np.sqrt(corrected_variance_march/len(march_data) + corrected_variance_feb/len(feb_data))

print(' Corrected Variance March', corrected_variance_march)
print(' Corrected Variance Feb', corrected_variance_feb)
print(' D bar',D_bar )
print(' t_statistic ', t_stat)

# threshold = 2.002465

# Accept Null Hypothesis

"""# Hypothesis tests for IA deaths

The below results show the value of the statistic calculated for each of the following hypotheis tests:
- Wald's test
- Z-test
- T-test
- Wald's 2 sample test
- Unpaired 2-sample T-test  

The Null hypothesis is rejected by all the above tests as the statistic calculated for each test is greater than the critical value for each of the above tests.

These tests are applicable as the number of samples can be assumed to be large. Thus, the mean is asymptotically normal using CLT. We estimate the true variance for the Z-test using the entire data.
"""

########################### Tests for IA death cases handle ###################################

handle = 'IA deaths'
mean_feb = np.mean(feb_data[handle])

## Using mean_feb as guess for mean_march ##

# Wald's test # 
# w = (theta_hat - theta_0)/ se_hat(theta_hat)
# theta_hat is estimator of theta

# Null Hypothesis : mean_march = mean_feb
# Alternate Hypothesis : mean_march != mean_feb

# Assuming the distribution of march data to be poisson. MLE_mean = Sample_mean

sample_mean_march = np.mean(march_data[handle])

standard_error_estimate = np.sqrt(sample_mean_march/len(march_data))

walds_statistic = np.abs((sample_mean_march - mean_feb)/standard_error_estimate)

print('Walds test')
print('MLE for March data ',sample_mean_march )
print('Guess mean ', mean_feb)
print('Standard Error ', standard_error_estimate)
print('Walds Statistic', walds_statistic )

# Reject Null Hypothesis(greater than 1.96)
print('########################################')


## Z-test ##

# z_statistic = (sample_mean - guess)/ root(true_variance/n)
# true variance = corrected sample standard deviation

sample_mean_full_data = np.mean(data[handle])

true_variance = np.sum(np.square(data[handle] - sample_mean_full_data))/(len(data)-1)

z_statistic = np.abs((sample_mean_march - mean_feb)/(np.sqrt(true_variance)/np.sqrt(len(march_data))))

print('Z test')
print(' True Variance ', true_variance)
print(' Sample Mean ',sample_mean_march )
print(' Guess ', mean_feb)
print(' z_statistic ', z_statistic)


# Reject Null Hypothesis(greater than 1.96)

print('########################################')


## t-test ##

# t_statistic = (sample_mean - guess)/ corrected_sample_standard_deviation/root(n)
corrected_sample_SD = np.sqrt(np.sum(np.square(march_data[handle] - sample_mean_march))/(len(march_data)-1))

t_statistic = np.abs((sample_mean_march - mean_feb)/(corrected_sample_SD/np.sqrt(len(march_data))))

print('T-test')
print(' Corrected Sample Standard Deviation', corrected_sample_SD)
print(' Sample Mean ',sample_mean_march )
print(' Guess ', mean_feb)
print(' t_statistic ', t_statistic)

# degrees of freedom : 28 + 31 -2 = 57
# threshold = 2.002465

# Reject Null Hypothesis

print('########################################')

## Walds - 2 sample test ##

# delta = mean_march - mean_feb 

# w_stat = delta_hat/ SE_hat(delta_hat)

# Assumption : Data is poisson distributed

# Null Hypothesis : delta = 0
# Alternate Hypothesis : delta != 0 


sample_mean_march = np.mean(march_data[handle])
sample_mean_feb = np.mean(feb_data[handle])

delta_hat = np.abs(sample_mean_march - sample_mean_feb)

SE_hat = np.sqrt(sample_mean_march/len(march_data) + sample_mean_feb/len(feb_data))

w_stat = np.abs(delta_hat/SE_hat)

print('Walds - 2 sample test')
print(' Standard Error Estimate', SE_hat)
print(' Delta Estimate ',delta_hat )
print(' w_stat ', w_stat)

# Reject Null Hypothesis

print('########################################')

## 2 sample unpaired t-test ##

# D_bar = X_bar - Y_bar
# t_stat = D_bar/root(corrected_var_x/n + corrected_var_y/m)

corrected_variance_march = np.sum(np.square(march_data[handle] - sample_mean_march))/(len(march_data)-1)
corrected_variance_feb =  np.sum(np.square(feb_data[handle] - sample_mean_feb))/(len(feb_data)-1)

D_bar = np.abs(sample_mean_march - sample_mean_feb)

t_stat = D_bar/np.sqrt(corrected_variance_march/len(march_data) + corrected_variance_feb/len(feb_data))

print('2 sample unpaired t-test ')
print(' Corrected Variance March', corrected_variance_march)
print(' Corrected Variance Feb', corrected_variance_feb)
print(' D bar',D_bar )
print(' t_statistic ', t_stat)

# threshold = 2.002465

# Reject Null Hypothesis

print('########################################')

"""# Hypothesis tests for ID confirmed

The below results show the value of the statistic calculated for each of the following hypotheis tests:
- Wald's test
- Z-test
- T-test
- Wald's 2 sample test
- Unpaired 2-sample T-test  

The Null hypothesis is rejected by all the above tests as the statistic calculated for each test is greater than the critical value for each of the above tests.

These tests are applicable as the number of samples can be assumed to be large. Thus, the mean is asymptotically normal using CLT. We estimate the true variance for the Z-test using the entire data.
"""

########################### Tests for ID confirmed cases handle ###################################

handle = 'ID confirmed'
mean_feb = np.mean(feb_data[handle])

## Using mean_feb as guess for mean_march ##

# Wald's test # 
# w = (theta_hat - theta_0)/ se_hat(theta_hat)
# theta_hat is estimator of theta

# Null Hypothesis : mean_march = mean_feb
# Alternate Hypothesis : mean_march != mean_feb

# Assuming the distribution of march data to be poisson. MLE_mean = Sample_mean

sample_mean_march = np.mean(march_data[handle])

standard_error_estimate = np.sqrt(sample_mean_march/len(march_data))

walds_statistic = np.abs((sample_mean_march - mean_feb)/standard_error_estimate)

print('Walds test')
print('MLE for March data ',sample_mean_march )
print('Guess mean ', mean_feb)
print('Standard Error ', standard_error_estimate)
print('Walds Statistic', walds_statistic )

# Reject Null Hypothesis(greater than 1.96)
print('########################################')


## Z-test ##

# z_statistic = (sample_mean - guess)/ root(true_variance/n)
# true variance = corrected sample standard deviation

sample_mean_full_data = np.mean(data[handle])

true_variance = np.sum(np.square(data[handle] - sample_mean_full_data))/(len(data)-1)

z_statistic = np.abs((sample_mean_march - mean_feb)/(np.sqrt(true_variance)/np.sqrt(len(march_data))))

print('Z test')
print(' True Variance ', true_variance)
print(' Sample Mean ',sample_mean_march )
print(' Guess ', mean_feb)
print(' z_statistic ', z_statistic)


# Reject Null Hypothesis(greater than 1.96)

print('########################################')


## t-test ##

# t_statistic = (sample_mean - guess)/ corrected_sample_standard_deviation/root(n)
corrected_sample_SD = np.sqrt(np.sum(np.square(march_data[handle] - sample_mean_march))/(len(march_data)-1))

t_statistic = np.abs((sample_mean_march - mean_feb)/(corrected_sample_SD/np.sqrt(len(march_data))))

print('T-test')
print(' Corrected Sample Standard Deviation', corrected_sample_SD)
print(' Sample Mean ',sample_mean_march )
print(' Guess ', mean_feb)
print(' t_statistic ', t_statistic)

# degrees of freedom : 28 + 31 -2 = 57
# threshold = 2.002465

# Reject Null Hypothesis

print('########################################')

## Walds - 2 sample test ##

# delta = mean_march - mean_feb 

# w_stat = delta_hat/ SE_hat(delta_hat)

# Assumption : Data is poisson distributed

# Null Hypothesis : delta = 0
# Alternate Hypothesis : delta != 0 


sample_mean_march = np.mean(march_data[handle])
sample_mean_feb = np.mean(feb_data[handle])

delta_hat = np.abs(sample_mean_march - sample_mean_feb)

SE_hat = np.sqrt(sample_mean_march/len(march_data) + sample_mean_feb/len(feb_data))

w_stat = np.abs(delta_hat/SE_hat)

print('Walds - 2 sample test')
print(' Standard Error Estimate', SE_hat)
print(' Delta Estimate ',delta_hat )
print(' w_stat ', w_stat)

# Reject Null Hypothesis

print('########################################')

## 2 sample unpaired t-test ##

# D_bar = X_bar - Y_bar
# t_stat = D_bar/root(corrected_var_x/n + corrected_var_y/m)

corrected_variance_march = np.sum(np.square(march_data[handle] - sample_mean_march))/(len(march_data)-1)
corrected_variance_feb =  np.sum(np.square(feb_data[handle] - sample_mean_feb))/(len(feb_data)-1)

D_bar = np.abs(sample_mean_march - sample_mean_feb)

t_stat = D_bar/np.sqrt(corrected_variance_march/len(march_data) + corrected_variance_feb/len(feb_data))

print('2 sample unpaired t-test ')
print(' Corrected Variance March', corrected_variance_march)
print(' Corrected Variance Feb', corrected_variance_feb)
print(' D bar',D_bar )
print(' t_statistic ', t_stat)

# threshold = 2.002465

# Reject Null Hypothesis

print('########################################')

"""# Hypothesis tests for ID deaths

The below results show the value of the statistic calculated for each of the following hypotheis tests:
- Wald's test
- Z-test
- T-test
- Wald's 2 sample test
- Unpaired 2-sample T-test  

The Null hypothesis is rejected by all the above tests as the statistic calculated for each test is greater than the critical value for each of the above tests.

These tests are applicable as the number of samples can be assumed to be large. Thus, the mean is asymptotically normal using CLT. We estimate the true variance for the Z-test using the entire data.
"""

########################### Tests for ID death cases handle ###################################

handle = 'ID deaths'
mean_feb = np.mean(feb_data[handle])

## Using mean_feb as guess for mean_march ##

# Wald's test # 
# w = (theta_hat - theta_0)/ se_hat(theta_hat)
# theta_hat is estimator of theta

# Null Hypothesis : mean_march = mean_feb
# Alternate Hypothesis : mean_march != mean_feb

# Assuming the distribution of march data to be poisson. MLE_mean = Sample_mean

sample_mean_march = np.mean(march_data[handle])

standard_error_estimate = np.sqrt(sample_mean_march/len(march_data))

walds_statistic = np.abs((sample_mean_march - mean_feb)/standard_error_estimate)

print('Walds test')
print('MLE for March data ',sample_mean_march )
print('Guess mean ', mean_feb)
print('Standard Error ', standard_error_estimate)
print('Walds Statistic', walds_statistic )

# Reject Null Hypothesis(greater than 1.96)
print('########################################')


## Z-test ##

# z_statistic = (sample_mean - guess)/ root(true_variance/n)
# true variance = corrected sample standard deviation

sample_mean_full_data = np.mean(data[handle])
true_variance = np.sum(np.square(data[handle] - sample_mean_full_data))/(len(data)-1)

z_statistic = np.abs((sample_mean_march - mean_feb)/(np.sqrt(true_variance)/np.sqrt(len(march_data))))

print('Z test')
print(' True Variance ', true_variance)
print(' Sample Mean ',sample_mean_march )
print(' Guess ', mean_feb)
print(' z_statistic ', z_statistic)


# Reject Null Hypothesis(greater than 1.96)

print('########################################')


## t-test ##

# t_statistic = (sample_mean - guess)/ corrected_sample_standard_deviation/root(n)
corrected_sample_SD = np.sqrt(np.sum(np.square(march_data[handle] - sample_mean_march))/(len(march_data)-1))

t_statistic = np.abs((sample_mean_march - mean_feb)/(corrected_sample_SD/np.sqrt(len(march_data))))

print('T-test')
print(' Corrected Sample Standard Deviation', corrected_sample_SD)
print(' Sample Mean ',sample_mean_march )
print(' Guess ', mean_feb)
print(' t_statistic ', t_statistic)

# degrees of freedom : 28 + 31 -2 = 57
# threshold = 2.002465

# Reject Null Hypothesis

print('########################################')

## Walds - 2 sample test ##

# delta = mean_march - mean_feb 

# w_stat = delta_hat/ SE_hat(delta_hat)

# Assumption : Data is poisson distributed

# Null Hypothesis : delta = 0
# Alternate Hypothesis : delta != 0 


sample_mean_march = np.mean(march_data[handle])
sample_mean_feb = np.mean(feb_data[handle])

delta_hat = np.abs(sample_mean_march - sample_mean_feb)

SE_hat = np.sqrt(sample_mean_march/len(march_data) + sample_mean_feb/len(feb_data))

w_stat = np.abs(delta_hat/SE_hat)

print('Walds - 2 sample test')
print(' Standard Error Estimate', SE_hat)
print(' Delta Estimate ',delta_hat )
print(' w_stat ', w_stat)

# Reject Null Hypothesis

print('########################################')

## 2 sample unpaired t-test ##

# D_bar = X_bar - Y_bar
# t_stat = D_bar/root(corrected_var_x/n + corrected_var_y/m)

corrected_variance_march = np.sum(np.square(march_data[handle] - sample_mean_march))/(len(march_data)-1)
corrected_variance_feb =  np.sum(np.square(feb_data[handle] - sample_mean_feb))/(len(feb_data)-1)

D_bar = np.abs(sample_mean_march - sample_mean_feb)

t_stat = D_bar/np.sqrt(corrected_variance_march/len(march_data) + corrected_variance_feb/len(feb_data))

print('2 sample unpaired t-test ')
print(' Corrected Variance March', corrected_variance_march)
print(' Corrected Variance Feb', corrected_variance_feb)
print(' D bar',D_bar )
print(' t_statistic ', t_stat)

# threshold = 2.002465

# Reject Null Hypothesis

print('########################################')